# Papers
Attention is all you need The OG paper that started all this
[LLM.int8()](https://arxiv.org/pdf/2208.07339.pdf) discusses the issue with large models and offers WQ soln
[GPTQ paper](https://arxiv.org/pdf/2210.17323.pdf) was able to 3bit quantise it 
[Effects of quantisation on LLMs, WQ vs WAQ](https://arxiv.org/pdf/2303.08302.pdf)

# Websites
[TinyML](https://tinyml.substack.com/) Exceptional summaries focusing on MLops, LLMs and tinyMLops
[Notebook on openai finetuning](https://colab.research.google.com/drive/1_sGhwQ5BrbNIt0NpCb3JSrjyc7zKLKDe?usp=sharingscrollTo=BRcpq-fbHOeA)
