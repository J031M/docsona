## 1. GLUE
General Language Understanding Evaluation benchmark. to measure the ability of a ml model to understand the meaning of text. it has tasks in it for checking certain specific abilities like check grammatic errors, etc. [[https://huggingface.co/datasets/glue]]
there is something called SuperGLUE too which is just like a bigger benchmark dataset based of GLUE.
## 2. NEWSROOM DATSET
apparently its a cornell datset for training and evaluating summarization systems. has like 1.3 million articles and their summarization for a range of topics. 
[[https://huggingface.co/datasets/newsroom]]
## 3. SciTLDR
really good for extreme summarization especially for scientific documents. its based on 2020 paper called "TLDR: Extreme Summarization of Scientific Documents".
[[https://huggingface.co/datasets/allenai/scitldr]]
## 4. CNN/dailymail 
apparently its used quite often to train summarization models for non scientific articles.
[[https://huggingface.co/datasets/cnn_dailymail]]
## 5. XSum
again very similar to above cnn dataset
[[https://huggingface.co/datasets/xsum]]